#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Prompt ç”Ÿæˆå™¨ RAG ç³»çµ±
åŸºæ–¼ LlamaIndex å’Œ Chroma çš„æ™ºèƒ½ Prompt æª¢ç´¢å’Œç”Ÿæˆç³»çµ±

åŠŸèƒ½ç‰¹è‰²ï¼š
- æ™ºèƒ½æ–‡æª”åˆ‡åˆ†ç­–ç•¥
- Chroma æ··åˆæ¶æ§‹ (ä¸‰å€‹å°ˆç”¨ collection)
- é›™è»Œæª¢ç´¢ç­–ç•¥ (ç„¡ä¸Šä¸‹æ–‡ vs æœ‰ä¸Šä¸‹æ–‡)
- Response Mode è‡ªå‹•é…ç½®
- ç”¨æˆ¶å‹å¥½çš„åˆ†é¡é¡¯ç¤º

ä½œè€…ï¼šAI Assistant
å‰µå»ºæ—¥æœŸï¼š2025-06-17
"""

import os
import pandas as pd
import numpy as np
import re
import uuid
import chromadb
from chromadb.config import Settings
from chromadb.utils import embedding_functions
from typing import List, Dict, Any, Optional

# LlamaIndex å°å…¥
from llama_index.core import VectorStoreIndex, StorageContext
from llama_index.core.schema import TextNode
from llama_index.vector_stores.chroma import ChromaVectorStore
from llama_index.embeddings.openai import OpenAIEmbedding
from llama_index.llms.openai import OpenAI
from llama_index.core import Settings as LlamaSettings

# ç³»çµ±é…ç½®
def setup_environment(openai_api_key: str):
    """è¨­ç½®ç’°å¢ƒå’Œ LlamaIndex é…ç½®"""
    os.environ["OPENAI_API_KEY"] = openai_api_key
    LlamaSettings.llm = OpenAI(model="gpt-3.5-turbo", temperature=0.1)
    LlamaSettings.embed_model = OpenAIEmbedding(model="text-embedding-ada-002")

# [åœ¨é€™è£¡æ’å…¥æ‰€æœ‰çš„é¡å®šç¾©ï¼šSmartChunkingStrategy, ChromaMixedArchitectureFixed, HybridSearchStrategy, PromptGeneratorRAGSystem]

def main():
    """ä¸»å‡½æ•¸ - ç³»çµ±åˆå§‹åŒ–å’Œæ¸¬è©¦"""
    # è¨­ç½® API Key
    api_key = input("è«‹è¼¸å…¥æ‚¨çš„ OpenAI API Key: ")
    setup_environment(api_key)
    
    # è¼‰å…¥è³‡æ–™
    dataset_path = input("è«‹è¼¸å…¥è³‡æ–™é›†è·¯å¾‘: ")
    df = pd.read_csv(dataset_path)
    
    print(f"è¼‰å…¥è³‡æ–™ï¼š{len(df)} ç­†è¨˜éŒ„")
    
    # åˆå§‹åŒ–ç³»çµ±
    chunking_strategy = SmartChunkingStrategy(max_tokens=1024)
    
    # åŸ·è¡Œåˆ‡åˆ†
    all_chunks, stats = process_all_records(df, chunking_strategy)
    print(f"åˆ‡åˆ†å®Œæˆï¼š{stats['total_chunks']} å€‹ chunks")
    
    # åˆå§‹åŒ– Chroma
    chroma_arch = ChromaMixedArchitectureFixed()
    if chroma_arch.initialize_chroma_client():
        if chroma_arch.create_collections():
            collection_chunks = chroma_arch.prepare_chunks_for_collections(all_chunks)
            if chroma_arch.add_chunks_to_collections(collection_chunks):
                print("âœ… Chroma æ•¸æ“šåº«åˆå§‹åŒ–å®Œæˆ")
            else:
                print("âŒ æ•¸æ“šè¼‰å…¥å¤±æ•—")
                return
        else:
            print("âŒ Collection å‰µå»ºå¤±æ•—")
            return
    else:
        print("âŒ Chroma å®¢æˆ¶ç«¯åˆå§‹åŒ–å¤±æ•—")
        return
    
    # åˆå§‹åŒ–æª¢ç´¢ç³»çµ±
    hybrid_search = HybridSearchStrategy(chroma_arch)
    rag_system = PromptGeneratorRAGSystem(chroma_arch, hybrid_search)
    
    print("ğŸ‰ ç³»çµ±åˆå§‹åŒ–å®Œæˆï¼")
    
    # æ¸¬è©¦æŸ¥è©¢
    while True:
        user_input = input("\nè«‹è¼¸å…¥æŸ¥è©¢ (è¼¸å…¥ 'quit' é€€å‡º): ")
        if user_input.lower() == 'quit':
            break
        
        context = input("è«‹è¼¸å…¥ä¸Šä¸‹æ–‡ (å¯é¸ï¼Œç›´æ¥æŒ‰ Enter è·³é): ")
        context = context if context.strip() else None
        
        result = rag_system.query(user_input, context)
        
        if "error" in result:
            print(f"âŒ æŸ¥è©¢å¤±æ•—ï¼š{result['error']}")
        else:
            print(f"âœ… æŸ¥è©¢æˆåŠŸï¼Œå ´æ™¯ï¼š{result['scenario']}")
            # é¡¯ç¤ºçµæœæ‘˜è¦...

if __name__ == "__main__":
    main()
